{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees for handwritten digit recognition\n",
    "\n",
    "This notebook demonstrates learning a [Decision Tree](https://en.wikipedia.org/wiki/Decision_tree_learning) using Spark's distributed implementation.  It gives the reader a better understanding of some critical [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_optimization) for the tree learning algorithm, using examples to demonstrate how tuning the hyperparameters can improve accuracy.\n",
    "\n",
    "**Background**: To learn more about Decision Trees, check out the resources at the end of this notebook.  [The visual description of ML and Decision Trees](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/) provides nice intuition helpful to understand this notebook, and [Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning) gives lots of details.\n",
    "\n",
    "**Data**: We use the classic MNIST handwritten digit recognition dataset.\n",
    "\n",
    "**Goal**: Our goal for our data is to learn how to recognize digits (0 - 9) from images of handwriting.  However, we will focus on understanding trees, not on this particular learning problem.\n",
    "\n",
    "**Takeaways**: Decision Trees take several hyperparameters which can affect the accuracy of the learned model.  There is no one \"best\" setting for these for all datasets.  To get the optimal accuracy, we need to tune these hyperparameters based on our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST training and test datasets\n",
    "\n",
    "Our datasets are vectors of pixels representing images of handwritten digits.  For example:\n",
    "\n",
    "![Image of a digit](http://training.databricks.com/databricks_guide/digit.png)\n",
    "![Image of all 10 digits](http://training.databricks.com/databricks_guide/MNIST-small.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These datasets are stored in the popular LibSVM dataset format.  We will load them using MLlib's LibSVM dataset reader utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">13</span><span class=\"ansired\">]: </span>DataFrame[label: double, features: vector]\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training = spark.read.format(\"libsvm\").load(\"/databricks-datasets/mnist-digits/data-001/mnist-digits-train.txt\")\n",
    "test = spark.read.format(\"libsvm\").load(\"/databricks-datasets/mnist-digits/data-001/mnist-digits-test.txt\")\n",
    "\n",
    "training.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">60000\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, DecisionTreeClassificationModel\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer \n",
    "\n",
    "print(training.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1\n",
    "\n",
    "Your task is to use the links provided in the Piazza post to learn about Spark, ML Pipelines and Decision Trees and move on to implementing those in this notebook.\n",
    "\n",
    "**Make sure you use the Spark ML Pipeline to carry out your workflow - otherwise using Spark would be no different from using SKLearn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 \n",
    "\n",
    "Answer the following questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. What do the parameters of the Decision Tree do? How do they change the complexity of the tree (i.e. how can you tune them if the model is overfitting/underfitting)?\n",
    "\n",
    "Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. How does the Spark ML Pipeline speed-up the computation? Run the same model and evaluation / validation techniques on your own machine and compare the computation speed. Use SKLearn for the 5-fold cross-validator for hyper-parameter tuning on your machine, and compare its speed with the cross-validator on Databricks. \n",
    "\n",
    "Ans:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "name": "Lab_4",
  "notebookId": 2714451962724020
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
