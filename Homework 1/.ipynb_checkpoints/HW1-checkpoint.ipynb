{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework we'll be looking at the data released by the State Bank of Pakistan. The data is the daily bank-wise and donor-wise receipts of the fund for the Daimer Bhasha and Mohmand Dam. You can find them in the following link: http://www.sbp.org.pk/notifications/FD/DamFund/Damfund.htm. Take a moment to look around the data and try to figure out what the possible challenges could be.\n",
    "\n",
    "The main purpose of this homework is to teach how to scrape data from the web, clean it, and import it into Pandas for data analysis purposes. There are however some things to note:\n",
    "1. As you can tell, the data is in PDF form. PDF is the most difficult to handle data format and if you get extremely broken CSV files, there isn't a need to worry, that's where the cleaning part comes in.\n",
    "2. We'll be using an API to convert the data from PDF to CSV, and then from CSV to Pandas. There are, however, other ways to do this. The reason we wanted to do this method is two-fold\n",
    "    * It will teach you how to communicate with APIs using Python, which will be a useful skill when you want to deploy your data models as an API so that it can work with other APIs that need those data models. Moreover, a lot of data you get in the real world is from APIs. \n",
    "    * The CSV will be extremely inconsistent, so it will give you immense practice with using regular-expressions, which are extremely important in the Data Science tool-kit.\n",
    "    \n",
    "Submit the notebooks in a similar format to the Labs: print the relevant output in each cell **only if it has an output. The initial scraping and converting does not have any output**, and name the notebooks as:\n",
    "**rollnumber_HW1.ipynb** for e.g **20100237_H1.ipynb**\n",
    "\n",
    "Please make sure you complete full parts (denoted by a Header each in this notebook) as the grading will be based on parts. Needless to say, do not copy someone else's code. In most Data Science careers, the main skill is not how good you are at coding, but how well you are able to use the tools at your disposal and what inferences you are able to make with the information that you have. Thus, while you might be able to do the HW by looking at someone else's code, unless you go through the actual thought process, you won't learn a lot.\n",
    "\n",
    "We'll be using a lot of libraries in this tutorial, make sure you go through them so you understand what they are used for.\n",
    "\n",
    "**NOTE: If you are more comfortable doing so, as I am, you can do the assignment on your preferred text editor on simple Python and then write the code neatly in a notebook.** Personally, I find Sublime/Vim easier to use than Jupyter, mostly since a lot of shortcuts there make coding much easier, while here the shortcuts are more about navigation and controlling your cells.\n",
    "\n",
    "**The homework is to be done in pairs of 2.** \n",
    "\n",
    "**Naming convention: rollnumber1_rollnumber2_HW1.ipynb**\n",
    "\n",
    "Total Marks: 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Getting the Data\n",
    "\n",
    "You can have a look at the data through the link given above. Download a few PDF files and go through the data to see what it looks like. How many columns are there, each, in the PDF files? Are there any inconsistencies? Any particular values that pop out that would need to be taken care of later in your cleaning? Think of all these questions when going through the initial PDF because they will prove really helpful when you can not figure out why there are so many \"NaN\" values in your final DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Data Scraping              \n",
    "Marks: 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using what is called the *requests* model to get an HTML page, and then use *BeautifulSoup* to parse that HTML page such that we are able to to derive the appropriate information from it. I recommend you go through the documentation of each to learn more about how to use the libraries. \n",
    "\n",
    "* [Requests Documentation](http://docs.python-requests.org/en/master/)\n",
    "* [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "* [BeautifulSoup + Requests Tutorial](https://www.pythonforbeginners.com/python-on-the-web/web-scraping-with-beautifulsoup)\n",
    "* [BeautifulSoup](https://medium.freecodecamp.org/how-to-scrape-websites-with-python-and-beautifulsoup-5946935d93fe) Note that this tutorial is more detailed. I would highly recommend you go through this as well even though the library used here is urllib2 instead of requests (which you can do as well!). It also links to more web-scraping libraries like Scrapy for more complicated scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "\n",
    "# os is being imported so you can make a new directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write code here that will:\n",
    "    # Open each PDF link\n",
    "    # Save the PDF in a directory in the same folder \n",
    "    \n",
    "    \n",
    "# Your code goes here #\n",
    "\n",
    "url = \"www.sbp.org.pk/notifications/FD/DamFund/Damfund.htm\"\n",
    "base_pdf_url = \"http://\" + (url.replace('/Damfund.htm', ''))\n",
    "cwd = os.getcwd()\n",
    "\n",
    "r  = requests.get(\"http://\" +url)\n",
    "\n",
    "data = r.text\n",
    "\n",
    "soup = BeautifulSoup(data, \"html.parser\")\n",
    "\n",
    "pdf_links = []\n",
    "\n",
    "for link in soup.find_all('a'):\n",
    "\tif 'pdf' in link.get('href'):\n",
    "\t\tpdf_links.append(base_pdf_url + '/' + link.get('href'))\n",
    "\n",
    "num_of_links = len(pdf_links)\n",
    "\n",
    "if not os.path.exists('all_pdfs'):\n",
    "\t    os.makedirs('all_pdfs')\n",
    "\n",
    "for i in xrange(0, num_of_links):\n",
    "\tr = requests.get(pdf_links[i], stream=True)\n",
    "\tmatch = re.search(r'\\d{2}-\\d{2}-\\d{4}', pdf_links[i])\n",
    "\tfilename = match.group()\n",
    "\twith open(cwd + '/all_pdfs/' + filename + '.pdf', 'wb') as f:\n",
    "\t\tf.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Converting from PDF to CSV\n",
    "Marks: 15\n",
    "\n",
    "You have two possible options between deciding what API to use for the conversion task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first option is communicating with an API called [Zamzar](https://www.zamzar.com) to send each PDF, ask them to convert it into CSV, and then download the converted CSV. They provide sample code to do everything from generating a simple request to starting a conversion job, checking for completion, and then downloading the finished file. You can find this information on the [Zamzar Documentation](https://developers.zamzar.com/docs) page.\n",
    "\n",
    "**Important Information: **\n",
    "\n",
    "The API only provides 100 points of free conversion, and each PDF to CSV conversion costs 3 points, that means with one account you can only convert **33** PDFs. However, this also means you have very little room to play around with this API, unless you have an extra email-address, so you need to be very careful when coding to communicate with this API. \n",
    "\n",
    "Moreover, the API only keeps the converted files for one day with a free account, so make sure you do this part in one go.\n",
    "\n",
    "**Note: Using the Zamzar API grants a bonus of 10 marks. This will help if you are not able to complete this assignment, or it can be used up in a later assignment if you get 110/100 marks in this one.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another extremely simple API is the [PDF Tables](https://pdftables.com) API which is much simpler to use than the Zamzar API, however does not allow you to check the job for completion or for any intermediate steps. Moreover, this requires the installation of a library. Once again, they allow only 50 versions for free, but that is enough conversions for us. This [blog post](https://pdftables.com/blog/pdf-to-excel-with-python) will help you figure out how to convert the PDF to CSV using Python.\n",
    "\n",
    "The cons of this API is that it will not really teach you any proper API communcation through requests since you do not have to navigate through any requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.auth import HTTPBasicAuth\n",
    "# import config\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-0e689f646942>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mjob_ids\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'id'"
     ]
    }
   ],
   "source": [
    "pdfs_folder = './all_pdfs/*.pdf'\n",
    "api_key = []\n",
    "api_key.append('bfbfea064c0e6241ee3714e30f0c0aa6ebadf9de')\n",
    "api_key.append('c6dbc24c1a45d90f7a3bde7178401ffcceef8254')\n",
    "endpoint = \"https://sandbox.zamzar.com/v1/jobs\"\n",
    "if not os.path.exists('all_csvs'):\n",
    "\tos.makedirs('all_csvs')\n",
    "\n",
    "# You need a list to store all the job_ids from the response of posting the conversion job, \n",
    "# if you are using the Zamzar API\n",
    "job_ids = []\n",
    "key = 0\n",
    "\n",
    "# This piece of code shows you what glob does\n",
    "for file_name in glob.glob(pdfs_folder):\n",
    "    source_file = file_name\n",
    "    target_format = \"csv\"\n",
    "    file_content = {'source_file': open(source_file, 'rb')}\n",
    "    data_content = {'target_format': target_format}\n",
    "    response = requests.post(endpoint, data=data_content, files=file_content, auth=HTTPBasicAuth(api_key[key], ''))\n",
    "    result = response.json()\n",
    "    if 'errors' in result:\n",
    "    \terrors = result['errors']\n",
    "    \terrors_list = errors[0]\n",
    "    \tif errors_list['code'] == 25:\n",
    "    \t\tcontinue\n",
    "    if 'errors' in result:\n",
    "    \terrors = result['errors']\n",
    "    \terrors_list = errors[0]\n",
    "    \tif errors_list['code'] == 24:\n",
    "    \t\tkey += 1\n",
    "    \t\tresponse = requests.post(endpoint, data=data_content, files=file_content, auth=HTTPBasicAuth(api_key[key], ''))\n",
    "    \t\tresult = response.json()\n",
    "\n",
    "    job_ids.append(result['id'])\n",
    "\n",
    "key = 0\n",
    "for id in job_ids:\n",
    "\tendpoint = \"https://sandbox.zamzar.com/v1/jobs/{}\".format(id)\n",
    "\tresponse = requests.get(endpoint, auth=HTTPBasicAuth(api_key[key], ''))\n",
    "\tresult = response.json()\n",
    "\tif 'errors' in result:\n",
    "\t\te = result['errors']\n",
    "\t\terrors_list = e[0]\n",
    "\t\tif errors_list['code'] == 21:\n",
    "\t\t\tkey += 1\n",
    "    \t\tresponse = requests.get(endpoint, auth=HTTPBasicAuth(api_key[key], ''))\n",
    "    \t\tresult = response.json()\n",
    "\tstatus = result['status']\n",
    "\tif status == 'failed':\n",
    "\t\tcontinue\n",
    "\twhile(status == 'converting'):\n",
    "\t\tresponse = requests.get(endpoint, auth=HTTPBasicAuth(api_key[key], ''))\n",
    "\t\tresult = response.json()\n",
    "\t\tstatus = result['status']\n",
    "\ttemp = result['target_files']\n",
    "\tfile_id = temp[0]['id']\n",
    "\tfile_name = temp[0]['name']\n",
    "\n",
    "\tlocal_filename = './all_csvs/' + file_name\n",
    "\tendpoint = \"https://sandbox.zamzar.com/v1/files/{}/content\".format(file_id)\n",
    "\tresponse = requests.get(endpoint, stream=True, auth=HTTPBasicAuth(api_key[key], ''))\n",
    "\n",
    "\ttry:\n",
    "\t  with open(local_filename, 'wb') as f:\n",
    "\t    for chunk in response.iter_content(chunk_size=1024):\n",
    "\t      if chunk:\n",
    "\t        f.write(chunk)\n",
    "\t        f.flush()\n",
    "\n",
    "\n",
    "\texcept IOError:\n",
    "\t  print \"Error downloading CSV\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below this cell write the code to download the completed files. First check if a job_id's status is completed and wait until it is. After it has been completed, download the file and save it.\n",
    "\n",
    "The exact code required here is all in the documentation, the only additional task you have to do on your own is figure out a way to find out which file has just been received from the job_id, and name the local file.\n",
    "\n",
    "**Please look at the Example JSON response in the [documentation](https://developers.zamzar.com/docs) to learn how to figure out the filenames, job status etc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code goes here ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Parsing the CSV File\n",
    "Marks: 35\n",
    "\n",
    "This is perhaps the most difficult part of the assignment, you have to follow a similar strategy to what you did the Udacity Lab 1. You can not simply use Pandas read_csv since the conversion is not perfect and there will be rows with different number of columns, which Pandas does not take care of.\n",
    "\n",
    "### **Main Task:**\n",
    "* Write a function that parses a CSV into a Pandas DataFrame\n",
    "* Each DataFrame should consist of three columns with headers Bank, Donor_Name, and Amount\n",
    "* The date should be retrieved from the given filename \n",
    "* The Donor_Name can be NaN, as it is in a lot of cases. But try to retrieve as much information as possible\n",
    "* Remove all \"Page of\" rows\n",
    "* Don't include the header rows (e.g. \"SUPREME COURT FUND....\") into the DataFrame\n",
    "* The Amount should be converted into a Pandas numeric at the end\n",
    "\n",
    "### Other info:\n",
    "Some important resources for this part are (you can choose any one tutorial that you feel is easy to understand, they all cover roughly the same content):\n",
    "* [RegEx Tutorial 1](https://www.regular-expressions.info/)\n",
    "* [RegEx Tutorial 2](https://regexone.com/lesson/introduction_abcs)\n",
    "* [RegEx Tutorial 3](https://www.rexegg.com/)\n",
    "* [RegEx Cheatsheat](https://medium.com/factory-mind/regex-tutorial-a-simple-cheatsheet-by-examples-649dc1c3f285)\n",
    "* This [RegEx Editor](https://regex101.com/) is your best friend since you can test your expression separately on this\n",
    "\n",
    "You will probably have to use the CSV reader in order to get all the rows of the file. You can learn more about it using this [tutorial](https://www.alexkras.com/how-to-read-csv-file-in-python/).\n",
    "\n",
    "Some tips:\n",
    "* First find out how many columns are in each row\n",
    "* Print out rows which are longer than they should be (they should all be of length 3)\n",
    "* Try to find patterns in how the data is spread, and what common problems exist in all rows\n",
    "* Write some regex to try an extract the amount from the problem row and then:\n",
    "    * Put the amount as the third column\n",
    "    * Merge the rest of the string as a name of the donor in the 2nd column\n",
    "* Also check if the rows with 3 columns are correctly formatted or not, many of them would probably not be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re # To use regular expressions\n",
    "import pandas as pd\n",
    "\n",
    "example_file = './all_csvs/17-08-2018.csv' # Assuming the file is in the folder all_csvs and is named appropriately\n",
    "# This is one of the most problematic files which is why I have included this in the example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(filename):\n",
    "    # Complete this function\n",
    "    \n",
    "    with open(filename) as f:\n",
    "        reader = csv.reader(f)\n",
    "        data = [row for row in reader]\n",
    "        \n",
    "        ## Just an example of one way to use the CSV module\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember, remove headers and convert all amounts to Numeric; if it can't be converted it needs to be NaN\n",
    "def read_csv(filename):\n",
    "    headers = ['Bank', 'Donor_Name','Amount']\n",
    "    \n",
    "    raw_data = parser(filename)\n",
    "    \n",
    "    df = # Your code goes here\n",
    "    \n",
    "    return df\n",
    "    \n",
    "print read_csv(example_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Importing Full Dataset\n",
    "Marks: 10 \n",
    "\n",
    "The only additional task in this part is to:\n",
    "* Run the parser on all the files\n",
    "* For each file **add a 'Date' column, which should be inferred from the filename**\n",
    "* Concatenate each DataFrame into one large DataFrame. *Hint: concat*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('./all_csvs/*.csv')\n",
    "\n",
    "full_data = # Your code goes her\n",
    "\n",
    "\n",
    "print full_data.head()\n",
    "print full_data.shape\n",
    "print full_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Data Integrity Checks\n",
    "Marks: 20\n",
    "\n",
    "* How many NaN values are there in each column? Why are they there? \n",
    "* What are the maximum and minimum values, is there anything peculiar about the max values?\n",
    "* Are there any rows which are not NaN but should still be a different DataFrame altogether?\n",
    "* Should these problem rows be removed? Can they be useful in other ways?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
